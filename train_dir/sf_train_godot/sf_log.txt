[2022-05-13 14:22:05,802][41384] Initializing learners...
[2022-05-13 14:22:05,803][41384] Initializing the learner 0 for policy 0
[2022-05-13 14:22:05,810][41413] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:22:08,810][41413] LEARNER	pid 41413	parent 41384
[2022-05-13 14:22:08,811][41413] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:22:08,836][41413] Visible devices: 1
[2022-05-13 14:22:08,837][41413] Starting seed is not provided
[2022-05-13 14:22:08,838][41413] Waiting for the learner to initialize...
[2022-05-13 14:24:01,318][41632] Initializing learners...
[2022-05-13 14:24:01,319][41632] Initializing the learner 0 for policy 0
[2022-05-13 14:24:01,326][41664] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:24:04,326][41664] LEARNER	pid 41664	parent 41632
[2022-05-13 14:24:04,327][41664] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:24:04,352][41664] Visible devices: 1
[2022-05-13 14:24:04,353][41664] Starting seed is not provided
[2022-05-13 14:24:04,353][41664] Waiting for the learner to initialize...
[2022-05-13 14:24:39,733][41760] Initializing learners...
[2022-05-13 14:24:39,733][41760] Initializing the learner 0 for policy 0
[2022-05-13 14:24:39,740][41789] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:24:42,743][41789] LEARNER	pid 41789	parent 41760
[2022-05-13 14:24:42,744][41789] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:24:42,767][41789] Visible devices: 1
[2022-05-13 14:24:42,768][41789] Starting seed is not provided
[2022-05-13 14:24:42,769][41789] Waiting for the learner to initialize...
[2022-05-13 14:25:57,686][42033] Initializing learners...
[2022-05-13 14:25:57,686][42033] Initializing the learner 0 for policy 0
[2022-05-13 14:25:57,692][42062] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:26:00,696][42062] LEARNER	pid 42062	parent 42033
[2022-05-13 14:26:00,696][42062] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:26:00,719][42062] Visible devices: 1
[2022-05-13 14:26:00,720][42062] Starting seed is not provided
[2022-05-13 14:26:00,721][42062] Waiting for the learner to initialize...
[2022-05-13 14:32:48,432][42613] Initializing learners...
[2022-05-13 14:32:48,433][42613] Initializing the learner 0 for policy 0
[2022-05-13 14:32:48,439][42643] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:32:51,443][42643] LEARNER	pid 42643	parent 42613
[2022-05-13 14:32:51,443][42643] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:32:51,468][42643] Visible devices: 1
[2022-05-13 14:32:51,469][42643] Starting seed is not provided
[2022-05-13 14:32:51,469][42643] Waiting for the learner to initialize...
[2022-05-13 14:40:06,123][43150] Initializing learners...
[2022-05-13 14:40:06,123][43150] Initializing the learner 0 for policy 0
[2022-05-13 14:40:06,130][43179] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:40:09,134][43179] LEARNER	pid 43179	parent 43150
[2022-05-13 14:40:09,134][43179] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:40:09,160][43179] Visible devices: 1
[2022-05-13 14:40:09,163][43179] Starting seed is not provided
[2022-05-13 14:40:09,163][43179] Waiting for the learner to initialize...
[2022-05-13 14:44:26,769][43847] Initializing learners...
[2022-05-13 14:44:26,769][43847] Initializing the learner 0 for policy 0
[2022-05-13 14:44:26,776][43876] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:44:29,780][43876] LEARNER	pid 43876	parent 43847
[2022-05-13 14:44:29,780][43876] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:44:29,803][43876] Visible devices: 1
[2022-05-13 14:44:29,804][43876] Starting seed is not provided
[2022-05-13 14:44:29,805][43876] Waiting for the learner to initialize...
[2022-05-13 14:46:29,074][44140] Initializing learners...
[2022-05-13 14:46:29,074][44140] Initializing the learner 0 for policy 0
[2022-05-13 14:46:29,080][44169] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:46:32,083][44169] LEARNER	pid 44169	parent 44140
[2022-05-13 14:46:32,084][44169] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:46:32,107][44169] Visible devices: 1
[2022-05-13 14:46:32,108][44169] Starting seed is not provided
[2022-05-13 14:46:32,109][44169] Waiting for the learner to initialize...
[2022-05-13 14:48:53,110][44322] Initializing learners...
[2022-05-13 14:48:53,111][44322] Initializing the learner 0 for policy 0
[2022-05-13 14:48:53,117][44357] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:48:56,118][44357] LEARNER	pid 44357	parent 44322
[2022-05-13 14:48:56,119][44357] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:48:56,148][44357] Visible devices: 1
[2022-05-13 14:48:56,149][44357] Starting seed is not provided
[2022-05-13 14:48:56,149][44357] Waiting for the learner to initialize...
[2022-05-13 14:53:42,643][44930] Initializing learners...
[2022-05-13 14:53:42,643][44930] Initializing the learner 0 for policy 0
[2022-05-13 14:53:42,649][44959] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:53:45,652][44959] LEARNER	pid 44959	parent 44930
[2022-05-13 14:53:45,653][44959] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:53:45,675][44959] Visible devices: 1
[2022-05-13 14:53:45,676][44959] Starting seed is not provided
[2022-05-13 14:53:45,677][44959] Waiting for the learner to initialize...
[2022-05-13 14:53:45,677][44959] Num input channels: 32
[2022-05-13 14:55:43,262][45115] Initializing learners...
[2022-05-13 14:55:43,262][45115] Initializing the learner 0 for policy 0
[2022-05-13 14:55:43,269][45144] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:55:46,272][45144] LEARNER	pid 45144	parent 45115
[2022-05-13 14:55:46,273][45144] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:55:46,296][45144] Visible devices: 1
[2022-05-13 14:55:46,297][45144] Starting seed is not provided
[2022-05-13 14:55:46,298][45144] Waiting for the learner to initialize...
[2022-05-13 14:55:46,298][45144] Num input channels: 32
[2022-05-13 14:56:32,870][45332] Initializing learners...
[2022-05-13 14:56:32,870][45332] Initializing the learner 0 for policy 0
[2022-05-13 14:56:32,877][45361] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 14:56:35,881][45361] LEARNER	pid 45361	parent 45332
[2022-05-13 14:56:35,881][45361] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 14:56:35,903][45361] Visible devices: 1
[2022-05-13 14:56:35,904][45361] Starting seed is not provided
[2022-05-13 14:56:35,905][45361] Waiting for the learner to initialize...
[2022-05-13 14:56:35,905][45361] Num input channels: 32
[2022-05-13 15:00:09,488][45749] Initializing learners...
[2022-05-13 15:00:09,488][45749] Initializing the learner 0 for policy 0
[2022-05-13 15:00:09,494][45786] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 15:00:12,498][45786] LEARNER	pid 45786	parent 45749
[2022-05-13 15:00:12,499][45786] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 15:00:12,524][45786] Visible devices: 1
[2022-05-13 15:00:12,528][45786] Starting seed is not provided
[2022-05-13 15:00:12,528][45786] Waiting for the learner to initialize...
[2022-05-13 15:00:12,529][45786] Num input channels: 4
[2022-05-13 15:00:50,262][45889] Initializing learners...
[2022-05-13 15:00:50,263][45889] Initializing the learner 0 for policy 0
[2022-05-13 15:00:50,271][45939] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 15:00:53,274][45939] LEARNER	pid 45939	parent 45889
[2022-05-13 15:00:53,275][45939] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 15:00:53,300][45939] Visible devices: 1
[2022-05-13 15:00:53,304][45939] Starting seed is not provided
[2022-05-13 15:00:53,304][45939] Waiting for the learner to initialize...
[2022-05-13 15:00:53,305][45939] Num input channels: 4
[2022-05-13 15:01:16,597][45977] Initializing learners...
[2022-05-13 15:01:16,598][45977] Initializing the learner 0 for policy 0
[2022-05-13 15:01:16,605][46010] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 15:01:19,609][46010] LEARNER	pid 46010	parent 45977
[2022-05-13 15:01:19,609][46010] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 15:01:19,631][46010] Visible devices: 1
[2022-05-13 15:01:19,633][46010] Starting seed is not provided
[2022-05-13 15:01:19,633][46010] Waiting for the learner to initialize...
[2022-05-13 15:01:19,634][46010] Num input channels: 4
[2022-05-13 15:01:40,954][46044] Initializing learners...
[2022-05-13 15:01:40,954][46044] Initializing the learner 0 for policy 0
[2022-05-13 15:01:40,962][46073] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 15:01:43,965][46073] LEARNER	pid 46073	parent 46044
[2022-05-13 15:01:43,966][46073] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 15:01:43,992][46073] Visible devices: 1
[2022-05-13 15:01:43,995][46073] Starting seed is not provided
[2022-05-13 15:01:43,995][46073] Waiting for the learner to initialize...
[2022-05-13 15:01:43,996][46073] Num input channels: 4
[2022-05-13 15:02:00,242][46176] Initializing learners...
[2022-05-13 15:02:00,243][46176] Initializing the learner 0 for policy 0
[2022-05-13 15:02:00,250][46205] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 15:02:03,254][46205] LEARNER	pid 46205	parent 46176
[2022-05-13 15:02:03,254][46205] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 15:02:03,279][46205] Visible devices: 1
[2022-05-13 15:02:03,280][46205] Starting seed is not provided
[2022-05-13 15:02:03,280][46205] Waiting for the learner to initialize...
[2022-05-13 15:02:03,281][46205] Num input channels: 4
[2022-05-13 15:02:03,346][46205] Encoder output size: 256
[2022-05-13 15:02:04,169][46205] No checkpoints found
[2022-05-13 15:02:04,170][46205] Did not load from checkpoint, starting from scratch!
[2022-05-13 15:02:04,170][46205] Broadcast model weights for model version 0
[2022-05-13 15:02:04,172][46205] Learner 0 initialized
[2022-05-13 15:02:04,173][46176] Initializing policy workers...
[2022-05-13 15:02:04,174][46176] Initializing policy worker 0 for policy 0
[2022-05-13 15:02:04,179][46176] Initializing policy worker 1 for policy 0
[2022-05-13 15:02:04,182][46176] Initializing actors...
[2022-05-13 15:02:04,187][46243] Initializing vector env runner 0...
[2022-05-13 15:02:04,187][46243] ACTOR worker 0	pid 46243	parent 46176
[2022-05-13 15:02:04,188][46243] Initializing envs for env runner 0...
[2022-05-13 15:02:04,195][46242] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:02:04,196][46241] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:02:04,199][46244] Initializing vector env runner 1...
[2022-05-13 15:02:04,199][46244] ACTOR worker 1	pid 46244	parent 46176
[2022-05-13 15:02:04,200][46244] Initializing envs for env runner 1...
[2022-05-13 15:02:04,200][46241] Visible devices: 1
[2022-05-13 15:02:04,200][46241] Initializing model on the policy worker 0-0...
[2022-05-13 15:02:04,200][46241] POLICY worker 0-0	pid 46241	parent 46176
[2022-05-13 15:02:04,215][46241] Num input channels: 4
[2022-05-13 15:02:04,215][46243] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:02:04,219][46242] Visible devices: 1
[2022-05-13 15:02:04,219][46242] Initializing model on the policy worker 0-1...
[2022-05-13 15:02:04,219][46242] POLICY worker 0-1	pid 46242	parent 46176
[2022-05-13 15:02:04,228][46244] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:02:04,235][46242] Num input channels: 4
[2022-05-13 15:02:04,279][46241] Encoder output size: 256
[2022-05-13 15:02:04,296][46242] Encoder output size: 256
[2022-05-13 15:02:05,204][46241] Initialized model on the policy worker 0-0!
[2022-05-13 15:02:05,205][46241] Min num requests: 1
[2022-05-13 15:02:05,226][46242] Initialized model on the policy worker 0-1!
[2022-05-13 15:02:05,226][46242] Min num requests: 1
[2022-05-13 15:02:07,823][46243] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 785, in _handle_reset
    policy_inputs = env_runner.reset(self.report_queue)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 577, in reset
    observations = e.reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 29, in reset
    obs = self.env.reset(**kwargs)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 54, in reset
    obs = self.env.reset()
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 124, in reset
    response["obs"] = self._process_obs(response["obs"])
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 108, in _process_obs
    sub[k] = self.decode_2d_obs_from_string(
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 265, in decode_2d_obs_from_string
    np.frombuffer(bytes.fromhex(hex_string), dtype=np.float16)
ValueError: cannot reshape array of size 2048 into shape (4,32,32)
[2022-05-13 15:02:07,924][46243] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0000, waiting: 0.0003, reset: 0.0156
[2022-05-13 15:02:07,979][46244] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 785, in _handle_reset
    policy_inputs = env_runner.reset(self.report_queue)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 577, in reset
    observations = e.reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 29, in reset
    obs = self.env.reset(**kwargs)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 54, in reset
    obs = self.env.reset()
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 124, in reset
    response["obs"] = self._process_obs(response["obs"])
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 108, in _process_obs
    sub[k] = self.decode_2d_obs_from_string(
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 265, in decode_2d_obs_from_string
    np.frombuffer(bytes.fromhex(hex_string), dtype=np.float16)
ValueError: cannot reshape array of size 2048 into shape (4,32,32)
[2022-05-13 15:02:08,080][46244] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0000, waiting: 0.0002, reset: 0.0162
[2022-05-13 15:02:08,186][46176] Worker 0 is stuck or failed (4.002). Reset!
[2022-05-13 15:02:08,186][46176] Status: False
[2022-05-13 15:02:08,191][46176] Worker 1 is stuck or failed (4.006). Reset!
[2022-05-13 15:02:08,191][46176] Status: False
[2022-05-13 15:02:08,199][46338] Initializing vector env runner 1...
[2022-05-13 15:02:08,199][46338] ACTOR worker 1	pid 46338	parent 46176
[2022-05-13 15:02:08,200][46338] Initializing envs for env runner 1...
[2022-05-13 15:02:08,203][46337] Initializing vector env runner 0...
[2022-05-13 15:02:08,203][46337] ACTOR worker 0	pid 46337	parent 46176
[2022-05-13 15:02:08,204][46337] Initializing envs for env runner 0...
[2022-05-13 15:02:08,224][46337] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:02:08,225][46338] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:02:08,233][46337] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:02:08,234][46338] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:02:08,335][46337] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:02:08,335][46338] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:02:09,231][46176] Worker 0 is stuck or failed (1.040). Reset!
[2022-05-13 15:02:09,232][46176] Status: False
[2022-05-13 15:02:09,238][46176] Worker 1 is stuck or failed (1.043). Reset!
[2022-05-13 15:02:09,238][46176] Status: False
[2022-05-13 15:02:09,243][46357] Initializing vector env runner 0...
[2022-05-13 15:02:09,244][46357] ACTOR worker 0	pid 46357	parent 46176
[2022-05-13 15:02:09,244][46357] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:02:09,246][46358] Initializing vector env runner 1...
[2022-05-13 15:02:09,246][46358] ACTOR worker 1	pid 46358	parent 46176
[2022-05-13 15:02:09,247][46358] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:02:09,345][46357] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002, reset: 0.0000
[2022-05-13 15:02:09,348][46358] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002, reset: 0.0000
[2022-05-13 15:02:10,242][46176] Worker 0 is stuck or failed (1.004). Reset!
[2022-05-13 15:02:10,242][46176] Status: False
[2022-05-13 15:02:10,248][46176] Worker 1 is stuck or failed (1.007). Reset!
[2022-05-13 15:02:10,249][46176] Status: False
[2022-05-13 15:02:10,255][46361] Initializing vector env runner 0...
[2022-05-13 15:02:10,255][46361] ACTOR worker 0	pid 46361	parent 46176
[2022-05-13 15:02:10,256][46361] Initializing envs for env runner 0...
[2022-05-13 15:02:10,263][46362] Initializing vector env runner 1...
[2022-05-13 15:02:10,263][46362] ACTOR worker 1	pid 46362	parent 46176
[2022-05-13 15:02:10,263][46362] Initializing envs for env runner 1...
[2022-05-13 15:02:10,283][46361] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:02:10,287][46362] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:02:10,288][46361] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:02:10,293][46362] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:02:10,389][46361] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:02:10,394][46362] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:02:11,233][46176] Worker 0 is stuck or failed (0.985). Reset!
[2022-05-13 15:02:11,233][46176] Status: False
[2022-05-13 15:02:11,238][46176] Worker 1 is stuck or failed (0.984). Reset!
[2022-05-13 15:02:11,238][46176] Status: False
[2022-05-13 15:02:11,251][46382] Initializing vector env runner 1...
[2022-05-13 15:02:11,251][46382] ACTOR worker 1	pid 46382	parent 46176
[2022-05-13 15:02:11,252][46382] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:02:11,252][46381] Initializing vector env runner 0...
[2022-05-13 15:02:11,253][46381] ACTOR worker 0	pid 46381	parent 46176
[2022-05-13 15:02:11,253][46381] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:02:11,352][46382] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:02:11,354][46381] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:02:12,257][46241] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:02:12,258][46242] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:02:12,459][46241] Policy worker avg. requests nan, timing: init: 1.0042, wait_policy_total: 7.0212, wait_policy: 0.0051, handle_policy_step: 0.0002, one_step: 0.0000
[2022-05-13 15:02:12,461][46242] Policy worker avg. requests nan, timing: init: 1.0074, wait_policy_total: 7.0009, wait_policy: 0.0051, handle_policy_step: 0.0002, one_step: 0.0000
[2022-05-13 15:04:41,627][46593] Initializing learners...
[2022-05-13 15:04:41,628][46593] Initializing the learner 0 for policy 0
[2022-05-13 15:04:41,636][46623] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 15:04:44,638][46623] LEARNER	pid 46623	parent 46593
[2022-05-13 15:04:44,639][46623] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 15:04:44,672][46623] Visible devices: 1
[2022-05-13 15:04:44,673][46623] Starting seed is not provided
[2022-05-13 15:04:44,673][46623] Waiting for the learner to initialize...
[2022-05-13 15:04:44,674][46623] Num input channels: 4
[2022-05-13 15:04:44,736][46623] Encoder output size: 256
[2022-05-13 15:04:45,602][46623] No checkpoints found
[2022-05-13 15:04:45,602][46623] Did not load from checkpoint, starting from scratch!
[2022-05-13 15:04:45,602][46623] Broadcast model weights for model version 0
[2022-05-13 15:04:45,604][46623] Learner 0 initialized
[2022-05-13 15:04:45,605][46593] Initializing policy workers...
[2022-05-13 15:04:45,606][46593] Initializing policy worker 0 for policy 0
[2022-05-13 15:04:45,611][46593] Initializing policy worker 1 for policy 0
[2022-05-13 15:04:45,616][46593] Initializing actors...
[2022-05-13 15:04:45,620][46657] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:04:45,620][46656] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:04:45,625][46656] Visible devices: 1
[2022-05-13 15:04:45,625][46656] Initializing model on the policy worker 0-0...
[2022-05-13 15:04:45,625][46656] POLICY worker 0-0	pid 46656	parent 46593
[2022-05-13 15:04:45,635][46656] Num input channels: 4
[2022-05-13 15:04:45,635][46658] Initializing vector env runner 0...
[2022-05-13 15:04:45,635][46658] ACTOR worker 0	pid 46658	parent 46593
[2022-05-13 15:04:45,635][46658] Initializing envs for env runner 0...
[2022-05-13 15:04:45,639][46659] Initializing vector env runner 1...
[2022-05-13 15:04:45,639][46659] ACTOR worker 1	pid 46659	parent 46593
[2022-05-13 15:04:45,639][46659] Initializing envs for env runner 1...
[2022-05-13 15:04:45,646][46657] Visible devices: 1
[2022-05-13 15:04:45,646][46657] Initializing model on the policy worker 0-1...
[2022-05-13 15:04:45,646][46657] POLICY worker 0-1	pid 46657	parent 46593
[2022-05-13 15:04:45,659][46658] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:04:45,661][46657] Num input channels: 4
[2022-05-13 15:04:45,662][46659] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:04:45,699][46656] Encoder output size: 256
[2022-05-13 15:04:45,722][46657] Encoder output size: 256
[2022-05-13 15:04:46,656][46656] Initialized model on the policy worker 0-0!
[2022-05-13 15:04:46,656][46656] Min num requests: 1
[2022-05-13 15:04:46,671][46657] Initialized model on the policy worker 0-1!
[2022-05-13 15:04:46,672][46657] Min num requests: 1
[2022-05-13 15:04:49,029][46658] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 785, in _handle_reset
    policy_inputs = env_runner.reset(self.report_queue)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 577, in reset
    observations = e.reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 29, in reset
    obs = self.env.reset(**kwargs)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 54, in reset
    obs = self.env.reset()
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 124, in reset
    response["obs"] = self._process_obs(response["obs"])
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 108, in _process_obs
    sub[k] = self.decode_2d_obs_from_string(
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 265, in decode_2d_obs_from_string
    np.frombuffer(bytes.fromhex(hex_string), dtype=np.float32)
ValueError: cannot reshape array of size 1024 into shape (4,32,32)
[2022-05-13 15:04:49,082][46659] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 785, in _handle_reset
    policy_inputs = env_runner.reset(self.report_queue)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 577, in reset
    observations = e.reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 29, in reset
    obs = self.env.reset(**kwargs)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 54, in reset
    obs = self.env.reset()
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 124, in reset
    response["obs"] = self._process_obs(response["obs"])
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 108, in _process_obs
    sub[k] = self.decode_2d_obs_from_string(
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 265, in decode_2d_obs_from_string
    np.frombuffer(bytes.fromhex(hex_string), dtype=np.float32)
ValueError: cannot reshape array of size 1024 into shape (4,32,32)
[2022-05-13 15:04:49,130][46658] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0000, waiting: 0.0001, reset: 0.0170
[2022-05-13 15:04:49,183][46659] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0000, waiting: 0.0002, reset: 0.0165
[2022-05-13 15:04:49,622][46593] Worker 0 is stuck or failed (4.003). Reset!
[2022-05-13 15:04:49,622][46593] Status: False
[2022-05-13 15:04:49,627][46593] Worker 1 is stuck or failed (4.006). Reset!
[2022-05-13 15:04:49,628][46593] Status: False
[2022-05-13 15:04:49,633][46748] Initializing vector env runner 0...
[2022-05-13 15:04:49,633][46748] ACTOR worker 0	pid 46748	parent 46593
[2022-05-13 15:04:49,633][46748] Initializing envs for env runner 0...
[2022-05-13 15:04:49,635][46749] Initializing vector env runner 1...
[2022-05-13 15:04:49,635][46749] ACTOR worker 1	pid 46749	parent 46593
[2022-05-13 15:04:49,636][46749] Initializing envs for env runner 1...
[2022-05-13 15:04:49,659][46749] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:04:49,661][46748] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:04:49,664][46749] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:04:49,669][46748] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:04:49,765][46749] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:04:49,770][46748] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:04:50,678][46593] Worker 0 is stuck or failed (1.051). Reset!
[2022-05-13 15:04:50,678][46593] Status: False
[2022-05-13 15:04:50,692][46593] Worker 1 is stuck or failed (1.060). Reset!
[2022-05-13 15:04:50,692][46593] Status: False
[2022-05-13 15:04:50,698][46768] Initializing vector env runner 0...
[2022-05-13 15:04:50,698][46768] ACTOR worker 0	pid 46768	parent 46593
[2022-05-13 15:04:50,699][46769] Initializing vector env runner 1...
[2022-05-13 15:04:50,699][46769] ACTOR worker 1	pid 46769	parent 46593
[2022-05-13 15:04:50,699][46768] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:04:50,700][46769] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:04:50,800][46768] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0008, waiting: 0.0008, reset: 0.0000
[2022-05-13 15:04:50,801][46769] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002, reset: 0.0000
[2022-05-13 15:04:51,696][46593] Worker 0 is stuck or failed (1.005). Reset!
[2022-05-13 15:04:51,697][46593] Status: False
[2022-05-13 15:04:51,703][46593] Worker 1 is stuck or failed (1.007). Reset!
[2022-05-13 15:04:51,704][46593] Status: False
[2022-05-13 15:04:51,708][46770] Initializing vector env runner 0...
[2022-05-13 15:04:51,708][46770] ACTOR worker 0	pid 46770	parent 46593
[2022-05-13 15:04:51,709][46770] Initializing envs for env runner 0...
[2022-05-13 15:04:51,711][46771] Initializing vector env runner 1...
[2022-05-13 15:04:51,712][46771] ACTOR worker 1	pid 46771	parent 46593
[2022-05-13 15:04:51,712][46771] Initializing envs for env runner 1...
[2022-05-13 15:04:51,733][46770] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:04:51,735][46771] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:04:51,740][46770] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:04:51,741][46771] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:04:51,842][46770] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:04:51,842][46771] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002
[2022-05-13 15:04:52,661][46593] Worker 0 is stuck or failed (0.958). Reset!
[2022-05-13 15:04:52,662][46593] Status: False
[2022-05-13 15:04:52,666][46593] Worker 1 is stuck or failed (0.960). Reset!
[2022-05-13 15:04:52,667][46593] Status: False
[2022-05-13 15:04:52,672][46791] Initializing vector env runner 1...
[2022-05-13 15:04:52,672][46791] ACTOR worker 1	pid 46791	parent 46593
[2022-05-13 15:04:52,672][46791] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:04:52,673][46790] Initializing vector env runner 0...
[2022-05-13 15:04:52,673][46790] ACTOR worker 0	pid 46790	parent 46593
[2022-05-13 15:04:52,674][46790] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:04:52,773][46791] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:04:52,774][46790] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:04:53,678][46593] Worker 0 is stuck or failed (1.012). Reset!
[2022-05-13 15:04:53,678][46593] Status: False
[2022-05-13 15:04:53,684][46593] Worker 1 is stuck or failed (1.013). Reset!
[2022-05-13 15:04:53,684][46593] Status: False
[2022-05-13 15:04:53,689][46792] Initializing vector env runner 0...
[2022-05-13 15:04:53,689][46792] ACTOR worker 0	pid 46792	parent 46593
[2022-05-13 15:04:53,690][46792] Initializing envs for env runner 0...
[2022-05-13 15:04:53,695][46793] Initializing vector env runner 1...
[2022-05-13 15:04:53,695][46793] ACTOR worker 1	pid 46793	parent 46593
[2022-05-13 15:04:53,696][46793] Initializing envs for env runner 1...
[2022-05-13 15:04:53,709][46792] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:04:53,713][46793] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:04:53,716][46792] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:04:53,721][46793] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:04:53,817][46792] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:04:53,823][46793] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:04:54,689][46593] Worker 0 is stuck or failed (1.005). Reset!
[2022-05-13 15:04:54,689][46593] Status: False
[2022-05-13 15:04:54,694][46593] Worker 1 is stuck or failed (1.005). Reset!
[2022-05-13 15:04:54,694][46593] Status: False
[2022-05-13 15:04:54,706][46813] Initializing vector env runner 1...
[2022-05-13 15:04:54,707][46813] ACTOR worker 1	pid 46813	parent 46593
[2022-05-13 15:04:54,707][46813] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:04:54,708][46812] Initializing vector env runner 0...
[2022-05-13 15:04:54,708][46812] ACTOR worker 0	pid 46812	parent 46593
[2022-05-13 15:04:54,709][46812] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:04:54,808][46813] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:04:54,810][46812] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:04:55,681][46593] Worker 0 is stuck or failed (0.987). Reset!
[2022-05-13 15:04:55,681][46593] Status: False
[2022-05-13 15:04:55,686][46593] Worker 1 is stuck or failed (0.987). Reset!
[2022-05-13 15:04:55,687][46593] Status: False
[2022-05-13 15:04:55,699][46814] Initializing vector env runner 0...
[2022-05-13 15:04:55,699][46814] ACTOR worker 0	pid 46814	parent 46593
[2022-05-13 15:04:55,700][46814] Initializing envs for env runner 0...
[2022-05-13 15:04:55,705][46815] Initializing vector env runner 1...
[2022-05-13 15:04:55,706][46815] ACTOR worker 1	pid 46815	parent 46593
[2022-05-13 15:04:55,706][46815] Initializing envs for env runner 1...
[2022-05-13 15:04:55,722][46814] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:04:55,725][46815] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:04:55,728][46814] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:04:55,730][46815] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:04:55,830][46814] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:04:55,832][46815] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:04:56,701][46656] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:04:56,703][46657] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:04:56,903][46656] Policy worker avg. requests nan, timing: init: 1.0309, wait_policy_total: 10.0034, wait_policy: 0.0051, handle_policy_step: 0.0003, one_step: 0.0000
[2022-05-13 15:04:56,905][46657] Policy worker avg. requests nan, timing: init: 1.0258, wait_policy_total: 9.9905, wait_policy: 0.0051, handle_policy_step: 0.0003, one_step: 0.0000
[2022-05-13 15:05:13,396][46853] Initializing learners...
[2022-05-13 15:05:13,397][46853] Initializing the learner 0 for policy 0
[2022-05-13 15:05:13,406][46888] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 15:05:16,409][46888] LEARNER	pid 46888	parent 46853
[2022-05-13 15:05:16,410][46888] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 15:05:16,433][46888] Visible devices: 1
[2022-05-13 15:05:16,449][46888] Starting seed is not provided
[2022-05-13 15:05:16,450][46888] Num input channels: 4
[2022-05-13 15:05:16,450][46888] Waiting for the learner to initialize...
[2022-05-13 15:05:16,508][46888] Encoder output size: 256
[2022-05-13 15:05:17,306][46888] No checkpoints found
[2022-05-13 15:05:17,306][46888] Did not load from checkpoint, starting from scratch!
[2022-05-13 15:05:17,306][46888] Broadcast model weights for model version 0
[2022-05-13 15:05:17,308][46888] Learner 0 initialized
[2022-05-13 15:05:17,309][46853] Initializing policy workers...
[2022-05-13 15:05:17,310][46853] Initializing policy worker 0 for policy 0
[2022-05-13 15:05:17,315][46853] Initializing policy worker 1 for policy 0
[2022-05-13 15:05:17,320][46853] Initializing actors...
[2022-05-13 15:05:17,324][46923] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:05:17,325][46922] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:05:17,326][46924] Initializing vector env runner 0...
[2022-05-13 15:05:17,326][46924] ACTOR worker 0	pid 46924	parent 46853
[2022-05-13 15:05:17,326][46924] Initializing envs for env runner 0...
[2022-05-13 15:05:17,331][46925] Initializing vector env runner 1...
[2022-05-13 15:05:17,331][46925] ACTOR worker 1	pid 46925	parent 46853
[2022-05-13 15:05:17,332][46925] Initializing envs for env runner 1...
[2022-05-13 15:05:17,348][46924] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:05:17,350][46925] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:05:17,350][46923] Visible devices: 1
[2022-05-13 15:05:17,351][46922] Visible devices: 1
[2022-05-13 15:05:17,351][46923] Initializing model on the policy worker 0-1...
[2022-05-13 15:05:17,351][46922] Initializing model on the policy worker 0-0...
[2022-05-13 15:05:17,351][46923] POLICY worker 0-1	pid 46923	parent 46853
[2022-05-13 15:05:17,351][46922] POLICY worker 0-0	pid 46922	parent 46853
[2022-05-13 15:05:17,358][46924] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:05:17,368][46922] Num input channels: 4
[2022-05-13 15:05:17,368][46923] Num input channels: 4
[2022-05-13 15:05:17,372][46925] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:05:17,428][46922] Encoder output size: 256
[2022-05-13 15:05:17,430][46923] Encoder output size: 256
[2022-05-13 15:05:17,460][46924] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0002
[2022-05-13 15:05:17,473][46925] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002
[2022-05-13 15:05:18,325][46853] Worker 0 is stuck or failed (1.002). Reset!
[2022-05-13 15:05:18,326][46853] Status: False
[2022-05-13 15:05:18,331][46853] Worker 1 is stuck or failed (1.006). Reset!
[2022-05-13 15:05:18,331][46853] Status: False
[2022-05-13 15:05:18,340][46996] Initializing vector env runner 0...
[2022-05-13 15:05:18,340][46996] ACTOR worker 0	pid 46996	parent 46853
[2022-05-13 15:05:18,340][46996] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:05:18,341][46997] Initializing vector env runner 1...
[2022-05-13 15:05:18,342][46997] ACTOR worker 1	pid 46997	parent 46853
[2022-05-13 15:05:18,342][46997] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:05:18,367][46923] Initialized model on the policy worker 0-1!
[2022-05-13 15:05:18,367][46923] Min num requests: 1
[2022-05-13 15:05:18,367][46922] Initialized model on the policy worker 0-0!
[2022-05-13 15:05:18,367][46922] Min num requests: 1
[2022-05-13 15:05:18,441][46996] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:05:18,443][46997] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:05:19,335][46853] Worker 0 is stuck or failed (1.005). Reset!
[2022-05-13 15:05:19,336][46853] Status: False
[2022-05-13 15:05:19,340][46853] Worker 1 is stuck or failed (1.005). Reset!
[2022-05-13 15:05:19,340][46853] Status: False
[2022-05-13 15:05:19,347][46998] Initializing vector env runner 0...
[2022-05-13 15:05:19,347][46998] ACTOR worker 0	pid 46998	parent 46853
[2022-05-13 15:05:19,348][46998] Initializing envs for env runner 0...
[2022-05-13 15:05:19,354][46999] Initializing vector env runner 1...
[2022-05-13 15:05:19,354][46999] ACTOR worker 1	pid 46999	parent 46853
[2022-05-13 15:05:19,355][46999] Initializing envs for env runner 1...
[2022-05-13 15:05:19,370][46998] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:05:19,378][46998] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:05:19,380][46999] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:05:19,389][46999] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:05:19,480][46998] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:05:19,491][46999] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:05:20,344][46853] Worker 0 is stuck or failed (1.004). Reset!
[2022-05-13 15:05:20,344][46853] Status: False
[2022-05-13 15:05:20,348][46853] Worker 1 is stuck or failed (1.005). Reset!
[2022-05-13 15:05:20,349][46853] Status: False
[2022-05-13 15:05:20,357][47019] Initializing vector env runner 1...
[2022-05-13 15:05:20,358][47019] ACTOR worker 1	pid 47019	parent 46853
[2022-05-13 15:05:20,358][47019] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:05:20,359][47018] Initializing vector env runner 0...
[2022-05-13 15:05:20,359][47018] ACTOR worker 0	pid 47018	parent 46853
[2022-05-13 15:05:20,359][47018] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:05:20,459][47019] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:05:20,460][47018] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:05:21,352][46853] Worker 0 is stuck or failed (1.004). Reset!
[2022-05-13 15:05:21,353][46853] Status: False
[2022-05-13 15:05:21,358][46853] Worker 1 is stuck or failed (1.006). Reset!
[2022-05-13 15:05:21,358][46853] Status: False
[2022-05-13 15:05:21,367][47022] Initializing vector env runner 0...
[2022-05-13 15:05:21,367][47022] ACTOR worker 0	pid 47022	parent 46853
[2022-05-13 15:05:21,368][47022] Initializing envs for env runner 0...
[2022-05-13 15:05:21,378][47023] Initializing vector env runner 1...
[2022-05-13 15:05:21,378][47023] ACTOR worker 1	pid 47023	parent 46853
[2022-05-13 15:05:21,379][47023] Initializing envs for env runner 1...
[2022-05-13 15:05:21,389][47022] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:05:21,395][47023] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:05:21,398][47022] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:05:21,400][47023] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:05:21,500][47022] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0002
[2022-05-13 15:05:21,502][47023] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:05:22,370][46853] Worker 0 is stuck or failed (1.012). Reset!
[2022-05-13 15:05:22,370][46853] Status: False
[2022-05-13 15:05:22,375][46853] Worker 1 is stuck or failed (1.012). Reset!
[2022-05-13 15:05:22,375][46853] Status: False
[2022-05-13 15:05:22,382][47043] Initializing vector env runner 0...
[2022-05-13 15:05:22,383][47043] ACTOR worker 0	pid 47043	parent 46853
[2022-05-13 15:05:22,383][47043] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:05:22,384][47044] Initializing vector env runner 1...
[2022-05-13 15:05:22,384][47044] ACTOR worker 1	pid 47044	parent 46853
[2022-05-13 15:05:22,385][47044] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:05:22,484][47043] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:05:22,485][47044] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:05:23,385][46922] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:05:23,385][46923] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:05:23,587][46923] Policy worker avg. requests nan, timing: init: 1.0166, wait_policy_total: 4.9953, wait_policy: 0.0051, handle_policy_step: 0.0001, one_step: 0.0000
[2022-05-13 15:05:23,587][46922] Policy worker avg. requests nan, timing: init: 1.0167, wait_policy_total: 4.9934, wait_policy: 0.0051, handle_policy_step: 0.0002, one_step: 0.0000
[2022-05-13 15:06:13,591][47139] Initializing learners...
[2022-05-13 15:06:13,591][47139] Initializing the learner 0 for policy 0
[2022-05-13 15:06:13,601][47173] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 15:06:16,602][47173] LEARNER	pid 47173	parent 47139
[2022-05-13 15:06:16,603][47173] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 15:06:16,628][47173] Visible devices: 1
[2022-05-13 15:06:16,629][47173] Starting seed is not provided
[2022-05-13 15:06:16,630][47173] Waiting for the learner to initialize...
[2022-05-13 15:06:16,630][47173] Num input channels: 4
[2022-05-13 15:06:16,692][47173] Encoder output size: 256
[2022-05-13 15:06:17,519][47173] No checkpoints found
[2022-05-13 15:06:17,519][47173] Did not load from checkpoint, starting from scratch!
[2022-05-13 15:06:17,519][47173] Broadcast model weights for model version 0
[2022-05-13 15:06:17,522][47173] Learner 0 initialized
[2022-05-13 15:06:17,522][47139] Initializing policy workers...
[2022-05-13 15:06:17,523][47139] Initializing policy worker 0 for policy 0
[2022-05-13 15:06:17,527][47139] Initializing policy worker 1 for policy 0
[2022-05-13 15:06:17,530][47139] Initializing actors...
[2022-05-13 15:06:17,536][47210] Initializing vector env runner 0...
[2022-05-13 15:06:17,536][47210] ACTOR worker 0	pid 47210	parent 47139
[2022-05-13 15:06:17,537][47210] Initializing envs for env runner 0...
[2022-05-13 15:06:17,543][47209] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:06:17,544][47211] Initializing vector env runner 1...
[2022-05-13 15:06:17,544][47211] ACTOR worker 1	pid 47211	parent 47139
[2022-05-13 15:06:17,545][47211] Initializing envs for env runner 1...
[2022-05-13 15:06:17,549][47208] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:06:17,559][47210] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:06:17,564][47211] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:06:17,566][47209] Visible devices: 1
[2022-05-13 15:06:17,566][47209] Initializing model on the policy worker 0-1...
[2022-05-13 15:06:17,566][47209] POLICY worker 0-1	pid 47209	parent 47139
[2022-05-13 15:06:17,570][47208] Visible devices: 1
[2022-05-13 15:06:17,570][47208] Initializing model on the policy worker 0-0...
[2022-05-13 15:06:17,570][47208] POLICY worker 0-0	pid 47208	parent 47139
[2022-05-13 15:06:17,592][47209] Num input channels: 4
[2022-05-13 15:06:17,592][47208] Num input channels: 4
[2022-05-13 15:06:17,652][47209] Encoder output size: 256
[2022-05-13 15:06:17,656][47208] Encoder output size: 256
[2022-05-13 15:06:18,608][47209] Initialized model on the policy worker 0-1!
[2022-05-13 15:06:18,608][47209] Min num requests: 1
[2022-05-13 15:06:18,618][47208] Initialized model on the policy worker 0-0!
[2022-05-13 15:06:18,618][47208] Min num requests: 1
[2022-05-13 15:06:21,286][47210] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 785, in _handle_reset
    policy_inputs = env_runner.reset(self.report_queue)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 577, in reset
    observations = e.reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 29, in reset
    obs = self.env.reset(**kwargs)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 54, in reset
    obs = self.env.reset()
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 124, in reset
    response["obs"] = self._process_obs(response["obs"])
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 108, in _process_obs
    sub[k] = self.decode_2d_obs_from_string(
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 265, in decode_2d_obs_from_string
    np.frombuffer(bytes.fromhex(hex_string), dtype=np.float32)
ValueError: cannot reshape array of size 1024 into shape (4,32,32)
[2022-05-13 15:06:21,333][47211] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 785, in _handle_reset
    policy_inputs = env_runner.reset(self.report_queue)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 577, in reset
    observations = e.reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 29, in reset
    obs = self.env.reset(**kwargs)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 54, in reset
    obs = self.env.reset()
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 124, in reset
    response["obs"] = self._process_obs(response["obs"])
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 108, in _process_obs
    sub[k] = self.decode_2d_obs_from_string(
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 265, in decode_2d_obs_from_string
    np.frombuffer(bytes.fromhex(hex_string), dtype=np.float32)
ValueError: cannot reshape array of size 1024 into shape (4,32,32)
[2022-05-13 15:06:21,387][47210] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0000, waiting: 0.0002, reset: 0.0175
[2022-05-13 15:06:21,435][47211] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0000, waiting: 0.0001, reset: 0.0148
[2022-05-13 15:06:21,535][47139] Worker 0 is stuck or failed (4.002). Reset!
[2022-05-13 15:06:21,535][47139] Status: False
[2022-05-13 15:06:21,540][47139] Worker 1 is stuck or failed (4.006). Reset!
[2022-05-13 15:06:21,540][47139] Status: False
[2022-05-13 15:06:21,545][47300] Initializing vector env runner 0...
[2022-05-13 15:06:21,545][47300] ACTOR worker 0	pid 47300	parent 47139
[2022-05-13 15:06:21,545][47300] Initializing envs for env runner 0...
[2022-05-13 15:06:21,547][47301] Initializing vector env runner 1...
[2022-05-13 15:06:21,548][47301] ACTOR worker 1	pid 47301	parent 47139
[2022-05-13 15:06:21,548][47301] Initializing envs for env runner 1...
[2022-05-13 15:06:21,565][47300] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:06:21,570][47301] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:06:21,572][47300] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:06:21,577][47301] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:06:21,674][47300] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002
[2022-05-13 15:06:21,678][47301] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002
[2022-05-13 15:06:22,634][47139] Worker 0 is stuck or failed (1.094). Reset!
[2022-05-13 15:06:22,635][47139] Status: False
[2022-05-13 15:06:22,639][47139] Worker 1 is stuck or failed (1.096). Reset!
[2022-05-13 15:06:22,640][47139] Status: False
[2022-05-13 15:06:22,645][47320] Initializing vector env runner 0...
[2022-05-13 15:06:22,645][47320] ACTOR worker 0	pid 47320	parent 47139
[2022-05-13 15:06:22,646][47320] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:06:22,647][47321] Initializing vector env runner 1...
[2022-05-13 15:06:22,648][47321] ACTOR worker 1	pid 47321	parent 47139
[2022-05-13 15:06:22,648][47321] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:06:22,747][47320] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002, reset: 0.0000
[2022-05-13 15:06:22,749][47321] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0002, reset: 0.0000
[2022-05-13 15:06:23,644][47139] Worker 0 is stuck or failed (1.005). Reset!
[2022-05-13 15:06:23,644][47139] Status: False
[2022-05-13 15:06:23,647][47139] Worker 1 is stuck or failed (1.004). Reset!
[2022-05-13 15:06:23,648][47139] Status: False
[2022-05-13 15:06:23,655][47323] Initializing vector env runner 1...
[2022-05-13 15:06:23,656][47323] ACTOR worker 1	pid 47323	parent 47139
[2022-05-13 15:06:23,656][47323] Initializing envs for env runner 1...
[2022-05-13 15:06:23,663][47322] Initializing vector env runner 0...
[2022-05-13 15:06:23,663][47322] ACTOR worker 0	pid 47322	parent 47139
[2022-05-13 15:06:23,663][47322] Initializing envs for env runner 0...
[2022-05-13 15:06:23,680][47323] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:06:23,683][47322] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:06:23,688][47323] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:06:23,689][47322] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:06:23,789][47323] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002
[2022-05-13 15:06:23,791][47322] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:06:24,643][47139] Worker 0 is stuck or failed (0.996). Reset!
[2022-05-13 15:06:24,643][47139] Status: False
[2022-05-13 15:06:24,647][47139] Worker 1 is stuck or failed (0.996). Reset!
[2022-05-13 15:06:24,647][47139] Status: False
[2022-05-13 15:06:24,652][47342] Initializing vector env runner 0...
[2022-05-13 15:06:24,652][47342] ACTOR worker 0	pid 47342	parent 47139
[2022-05-13 15:06:24,652][47342] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:06:24,658][47343] Initializing vector env runner 1...
[2022-05-13 15:06:24,659][47343] ACTOR worker 1	pid 47343	parent 47139
[2022-05-13 15:06:24,659][47343] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:06:24,753][47342] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:06:24,760][47343] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:06:25,656][47139] Worker 0 is stuck or failed (1.010). Reset!
[2022-05-13 15:06:25,657][47139] Status: False
[2022-05-13 15:06:25,660][47139] Worker 1 is stuck or failed (1.010). Reset!
[2022-05-13 15:06:25,660][47139] Status: False
[2022-05-13 15:06:25,673][47344] Initializing vector env runner 0...
[2022-05-13 15:06:25,673][47344] ACTOR worker 0	pid 47344	parent 47139
[2022-05-13 15:06:25,673][47344] Initializing envs for env runner 0...
[2022-05-13 15:06:25,675][47345] Initializing vector env runner 1...
[2022-05-13 15:06:25,675][47345] ACTOR worker 1	pid 47345	parent 47139
[2022-05-13 15:06:25,676][47345] Initializing envs for env runner 1...
[2022-05-13 15:06:25,696][47345] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:06:25,697][47344] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:06:25,703][47345] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:06:25,705][47344] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:06:25,804][47345] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:06:25,807][47344] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:06:26,663][47139] Worker 0 is stuck or failed (1.003). Reset!
[2022-05-13 15:06:26,663][47139] Status: False
[2022-05-13 15:06:26,667][47139] Worker 1 is stuck or failed (1.004). Reset!
[2022-05-13 15:06:26,667][47139] Status: False
[2022-05-13 15:06:26,681][47364] Initializing vector env runner 0...
[2022-05-13 15:06:26,681][47364] ACTOR worker 0	pid 47364	parent 47139
[2022-05-13 15:06:26,682][47364] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:06:26,683][47365] Initializing vector env runner 1...
[2022-05-13 15:06:26,683][47365] ACTOR worker 1	pid 47365	parent 47139
[2022-05-13 15:06:26,684][47365] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:06:26,784][47364] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0002, waiting: 0.0002, reset: 0.0000
[2022-05-13 15:06:26,785][47365] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:06:27,669][47208] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:06:27,671][47209] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:06:27,871][47208] Policy worker avg. requests nan, timing: init: 1.0477, wait_policy_total: 9.0149, wait_policy: 0.0050, handle_policy_step: 0.0002, one_step: 0.0000
[2022-05-13 15:06:27,873][47209] Policy worker avg. requests nan, timing: init: 1.0423, wait_policy_total: 9.0259, wait_policy: 0.0051, handle_policy_step: 0.0002, one_step: 0.0000
[2022-05-13 15:09:25,867][47756] Initializing learners...
[2022-05-13 15:09:25,867][47756] Initializing the learner 0 for policy 0
[2022-05-13 15:09:25,875][47787] WARNING! It is recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks. I.e. set --kl_loss_coeff=1.0
[2022-05-13 15:09:28,878][47787] LEARNER	pid 47787	parent 47756
[2022-05-13 15:09:28,879][47787] Set environment var CUDA_VISIBLE_DEVICES to '0' for learner process 0
[2022-05-13 15:09:28,908][47787] Visible devices: 1
[2022-05-13 15:09:28,909][47787] Starting seed is not provided
[2022-05-13 15:09:28,909][47787] Waiting for the learner to initialize...
[2022-05-13 15:09:28,910][47787] Num input channels: 4
[2022-05-13 15:09:28,973][47787] Encoder output size: 256
[2022-05-13 15:09:29,798][47787] No checkpoints found
[2022-05-13 15:09:29,798][47787] Did not load from checkpoint, starting from scratch!
[2022-05-13 15:09:29,798][47787] Broadcast model weights for model version 0
[2022-05-13 15:09:29,801][47787] Learner 0 initialized
[2022-05-13 15:09:29,801][47756] Initializing policy workers...
[2022-05-13 15:09:29,803][47756] Initializing policy worker 0 for policy 0
[2022-05-13 15:09:29,808][47756] Initializing policy worker 1 for policy 0
[2022-05-13 15:09:29,813][47756] Initializing actors...
[2022-05-13 15:09:29,815][47821] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:09:29,816][47822] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0
[2022-05-13 15:09:29,819][47823] Initializing vector env runner 0...
[2022-05-13 15:09:29,819][47823] ACTOR worker 0	pid 47823	parent 47756
[2022-05-13 15:09:29,819][47823] Initializing envs for env runner 0...
[2022-05-13 15:09:29,824][47824] Initializing vector env runner 1...
[2022-05-13 15:09:29,824][47824] ACTOR worker 1	pid 47824	parent 47756
[2022-05-13 15:09:29,825][47824] Initializing envs for env runner 1...
[2022-05-13 15:09:29,842][47821] Visible devices: 1
[2022-05-13 15:09:29,842][47821] Initializing model on the policy worker 0-0...
[2022-05-13 15:09:29,842][47821] POLICY worker 0-0	pid 47821	parent 47756
[2022-05-13 15:09:29,842][47822] Visible devices: 1
[2022-05-13 15:09:29,842][47822] Initializing model on the policy worker 0-1...
[2022-05-13 15:09:29,843][47822] POLICY worker 0-1	pid 47822	parent 47756
[2022-05-13 15:09:29,843][47823] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:09:29,844][47824] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:09:29,859][47821] Num input channels: 4
[2022-05-13 15:09:29,859][47822] Num input channels: 4
[2022-05-13 15:09:29,921][47822] Encoder output size: 256
[2022-05-13 15:09:29,922][47821] Encoder output size: 256
[2022-05-13 15:09:30,875][47821] Initialized model on the policy worker 0-0!
[2022-05-13 15:09:30,875][47821] Min num requests: 1
[2022-05-13 15:09:30,883][47822] Initialized model on the policy worker 0-1!
[2022-05-13 15:09:30,883][47822] Min num requests: 1
[2022-05-13 15:09:33,095][47824] Decorrelating experience for 64 frames...
[2022-05-13 15:09:33,097][47823] Decorrelating experience for 96 frames...
[2022-05-13 15:09:35,096][47824] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 785, in _handle_reset
    policy_inputs = env_runner.reset(self.report_queue)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 590, in reset
    actor_state.set_trajectory_data(dict(obs=obs), self.rollout_step)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 156, in set_trajectory_data
    self.curr_traj_buffer.set_data(rollout_step, data)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/shared_buffers.py", line 240, in set_data
    self.set_data_func(self, index, new_data)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/shared_buffers.py", line 245, in set_data_func
    self.set_data_func(x[new_data_key], index, new_data_value)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/shared_buffers.py", line 245, in set_data_func
    self.set_data_func(x[new_data_key], index, new_data_value)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/shared_buffers.py", line 245, in set_data_func
    self.set_data_func(x[new_data_key], index, new_data_value)
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
[2022-05-13 15:09:35,197][47824] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0000, waiting: 0.0002, reset: 2.0304
[2022-05-13 15:09:35,886][47756] Worker 1 is stuck or failed (6.068). Reset!
[2022-05-13 15:09:35,887][47756] Status: False
[2022-05-13 15:09:35,895][47911] Initializing vector env runner 1...
[2022-05-13 15:09:35,896][47911] ACTOR worker 1	pid 47911	parent 47756
[2022-05-13 15:09:35,896][47911] Initializing envs for env runner 1...
[2022-05-13 15:09:35,922][47911] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:09:35,928][47911] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:09:36,030][47911] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:09:36,092][47823] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 785, in _handle_reset
    policy_inputs = env_runner.reset(self.report_queue)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 590, in reset
    actor_state.set_trajectory_data(dict(obs=obs), self.rollout_step)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 156, in set_trajectory_data
    self.curr_traj_buffer.set_data(rollout_step, data)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/shared_buffers.py", line 240, in set_data
    self.set_data_func(self, index, new_data)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/shared_buffers.py", line 245, in set_data_func
    self.set_data_func(x[new_data_key], index, new_data_value)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/shared_buffers.py", line 245, in set_data_func
    self.set_data_func(x[new_data_key], index, new_data_value)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/shared_buffers.py", line 245, in set_data_func
    self.set_data_func(x[new_data_key], index, new_data_value)
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
[2022-05-13 15:09:36,193][47823] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0000, waiting: 0.0002, reset: 3.0221
[2022-05-13 15:09:36,888][47756] Worker 0 is stuck or failed (7.072). Reset!
[2022-05-13 15:09:36,889][47756] Status: False
[2022-05-13 15:09:36,894][47756] Worker 1 is stuck or failed (1.003). Reset!
[2022-05-13 15:09:36,894][47756] Status: False
[2022-05-13 15:09:36,900][47921] Initializing vector env runner 0...
[2022-05-13 15:09:36,900][47921] ACTOR worker 0	pid 47921	parent 47756
[2022-05-13 15:09:36,901][47921] Initializing envs for env runner 0...
[2022-05-13 15:09:36,903][47922] Initializing vector env runner 1...
[2022-05-13 15:09:36,904][47922] ACTOR worker 1	pid 47922	parent 47756
[2022-05-13 15:09:36,904][47922] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:09:36,921][47921] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:09:36,929][47921] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:09:37,005][47922] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0002, reset: 0.0000
[2022-05-13 15:09:37,030][47921] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:09:37,899][47756] Worker 0 is stuck or failed (1.005). Reset!
[2022-05-13 15:09:37,899][47756] Status: False
[2022-05-13 15:09:37,904][47756] Worker 1 is stuck or failed (1.005). Reset!
[2022-05-13 15:09:37,904][47756] Status: False
[2022-05-13 15:09:37,908][47932] Initializing vector env runner 0...
[2022-05-13 15:09:37,908][47932] ACTOR worker 0	pid 47932	parent 47756
[2022-05-13 15:09:37,909][47932] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:09:37,911][47933] Initializing vector env runner 1...
[2022-05-13 15:09:37,911][47933] ACTOR worker 1	pid 47933	parent 47756
[2022-05-13 15:09:37,912][47933] Initializing envs for env runner 1...
[2022-05-13 15:09:37,930][47933] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:09:37,934][47933] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:09:38,009][47932] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:09:38,036][47933] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:09:38,908][47756] Worker 0 is stuck or failed (1.004). Reset!
[2022-05-13 15:09:38,908][47756] Status: False
[2022-05-13 15:09:38,912][47756] Worker 1 is stuck or failed (1.004). Reset!
[2022-05-13 15:09:38,912][47756] Status: False
[2022-05-13 15:09:38,923][47943] Initializing vector env runner 0...
[2022-05-13 15:09:38,923][47943] ACTOR worker 0	pid 47943	parent 47756
[2022-05-13 15:09:38,924][47943] Initializing envs for env runner 0...
[2022-05-13 15:09:38,927][47944] Initializing vector env runner 1...
[2022-05-13 15:09:38,927][47944] ACTOR worker 1	pid 47944	parent 47756
[2022-05-13 15:09:38,928][47944] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:09:38,945][47943] Worker 0 uses CPU cores [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2022-05-13 15:09:38,954][47943] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:09:39,029][47944] Env runner 1, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:09:39,055][47943] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:09:39,911][47756] Worker 0 is stuck or failed (0.999). Reset!
[2022-05-13 15:09:39,911][47756] Status: False
[2022-05-13 15:09:39,915][47756] Worker 1 is stuck or failed (1.000). Reset!
[2022-05-13 15:09:39,916][47756] Status: False
[2022-05-13 15:09:39,923][47954] Initializing vector env runner 0...
[2022-05-13 15:09:39,923][47954] ACTOR worker 0	pid 47954	parent 47756
[2022-05-13 15:09:39,924][47954] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 892, in _run
    self._handle_reset()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 784, in _handle_reset
    for split_idx, env_runner in enumerate(self.env_runners):
TypeError: 'NoneType' object is not iterable
[2022-05-13 15:09:39,930][47955] Initializing vector env runner 1...
[2022-05-13 15:09:39,930][47955] ACTOR worker 1	pid 47955	parent 47756
[2022-05-13 15:09:39,931][47955] Initializing envs for env runner 1...
[2022-05-13 15:09:39,952][47955] Worker 1 uses CPU cores [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2022-05-13 15:09:39,961][47955] Unknown exception in rollout worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 876, in _run
    self._init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 738, in _init
    env_runner.init()
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/actor_worker.py", line 358, in init
    env = make_env_func(self.cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/appo_utils.py", line 38, in make_env_func
    env = create_env(cfg.env, cfg=cfg, env_config=env_config)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/envs/create_env.py", line 22, in create_env
    env = env_registry_entry.make_env_func(full_env_name, cfg=cfg, env_config=env_config)
  File "zeb_try_sample_factory.py", line 40, in make_godot_env_func
    return StableBaselinesGodotEnv(env_path="envs/builds/VirtualCamera/virtual_camera.x86_64", port=port, seed=seed, show_window=True)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/wrappers/stable_baselines_wrapper.py", line 9, in __init__
    self.env = GodotEnv(env_path=env_path, port=port, seed=seed, show_window=show_window, framerate=5)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 46, in __init__
    self.connection = self._start_server(self.port)
  File "/home/edward/work/godot_rl_agents/godot_rl_agents/core/godot_env.py", line 186, in _start_server
    sock.bind(server_address)
OSError: [Errno 98] Address already in use
[2022-05-13 15:09:40,025][47954] Env runner 0, CPU aff. [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001, reset: 0.0000
[2022-05-13 15:09:40,063][47955] Env runner 1, CPU aff. [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], rollouts 0: timing wait_actor: 0.0001, waiting: 0.0001
[2022-05-13 15:09:40,923][47821] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:09:40,925][47822] Unknown exception on policy worker
Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "faster_fifo.pyx", line 188, in faster_fifo.Queue.get_many
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/sample_factory/algorithms/appo/policy_worker.py", line 236, in _run
    policy_requests = self.policy_queue.get_many(timeout=0.005)
  File "/home/edward/miniconda3/envs/gdrl_conda/lib/python3.8/site-packages/ray/worker.py", line 1072, in sigterm_handler
    sys.exit(signum)
SystemExit: 15
[2022-05-13 15:09:41,124][47821] Policy worker avg. requests nan, timing: init: 1.0333, wait_policy_total: 10.0088, wait_policy: 0.0051, handle_policy_step: 0.0002, one_step: 0.0000
[2022-05-13 15:09:41,127][47822] Policy worker avg. requests nan, timing: init: 1.0404, wait_policy_total: 10.0007, wait_policy: 0.0051, handle_policy_step: 0.0003, one_step: 0.0000
